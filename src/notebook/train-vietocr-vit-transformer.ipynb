{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11687482,"sourceType":"datasetVersion","datasetId":7335578},{"sourceId":374661,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":309733,"modelId":330104}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchvision\n!pip install torchaudio\n!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T02:36:11.024937Z","iopub.execute_input":"2025-05-06T02:36:11.025587Z","iopub.status.idle":"2025-05-06T02:37:32.653118Z","shell.execute_reply.started":"2025-05-06T02:36:11.025546Z","shell.execute_reply":"2025-05-06T02:37:32.652375Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.5.1+cu124)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->torchvision)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->torchvision)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->torchvision)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->torchvision)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->torchvision)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->torchvision)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->torchvision)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.5.1+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->torchaudio) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1->torchaudio) (3.0.2)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 1. Model","metadata":{}},{"cell_type":"code","source":"from einops import rearrange\nimport math\nimport torch\nfrom torch import nn\n\nclass LanguageTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        d_model,\n        nhead,\n        num_encoder_layers,\n        num_decoder_layers,\n        dim_feedforward,\n        max_seq_length,\n        pos_dropout,\n        trans_dropout,\n    ):\n        super().__init__()\n        self.d_model = d_model\n        self.embed_tgt = nn.Embedding(vocab_size, d_model)\n        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)\n        self.transformer = nn.Transformer(\n            d_model,\n            nhead,\n            num_encoder_layers,\n            num_decoder_layers,\n            dim_feedforward,\n            trans_dropout,\n            batch_first=True  # Ensure batch_first is set\n        )\n        self.fc = nn.Linear(d_model, vocab_size)\n\n    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n        tgt_mask = self.gen_nopeek_mask(tgt.shape[1]).to(src.device)\n        src = self.pos_enc(src * math.sqrt(self.d_model))\n        tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))\n        output = self.transformer(\n            src, tgt,\n            tgt_mask=tgt_mask,\n            src_key_padding_mask=src_key_padding_mask,\n            tgt_key_padding_mask=tgt_key_padding_mask,\n            memory_key_padding_mask=memory_key_padding_mask,\n        )\n        return self.fc(output)\n\n    def gen_nopeek_mask(self, length):\n        mask = (torch.triu(torch.ones(length, length)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, 0.0)\n        return mask\n\n    def forward_encoder(self, src):\n        src = self.pos_enc(src * math.sqrt(self.d_model))\n        memory = self.transformer.encoder(src)\n        return memory\n\n    def forward_decoder(self, tgt, memory):\n        tgt_mask = self.gen_nopeek_mask(tgt.shape[1]).to(tgt.device)\n        # Embed and add positional encoding\n        tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))\n        # Decoder expects batch-first tensors (N, T, E)\n        output = self.transformer.decoder(tgt, memory, tgt_mask=tgt_mask)\n        return self.fc(output), memory\n    def expand_memory(self, memory, beam_size):\n        # Expand memory along the BATCH dimension (dim=0)\n        return memory.repeat(beam_size, 1, 1)  # (batch*beam, seq_len, d_model)\n    def get_memory(self, memory, i):\n        # Select the i-th batch element (batch-first)\n        return memory[i:i+1, :, :]  # (1, seq_len, d_model)\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=100):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)  # Shape: (max_len, 1, d_model)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # x shape: (batch_size, seq_len, d_model)\n        seq_len = x.size(1)\n        pe = self.pe[:seq_len, :].squeeze(1)  # (seq_len, d_model)\n        x = x + pe.unsqueeze(0)  # Add (1, seq_len, d_model) to (batch, seq_len, d_model)\n        return self.dropout(x)\n\n\nclass LearnedPositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=100):\n        super(LearnedPositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        self.pos_embed = nn.Embedding(max_len, d_model)\n        self.layernorm = LayerNorm(d_model)\n\n    def forward(self, x):\n        seq_len = x.size(0)\n        pos = torch.arange(seq_len, dtype=torch.long, device=x.device)\n        pos = pos.unsqueeze(-1).expand(x.size()[:2])\n        x = x + self.pos_embed(pos)\n        return self.dropout(self.layernorm(x))\n\n\nclass LayerNorm(nn.Module):\n    \"A layernorm module in the TF style (epsilon inside the square root).\"\n\n    def __init__(self, d_model, variance_epsilon=1e-12):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(d_model))\n        self.beta = nn.Parameter(torch.zeros(d_model))\n        self.variance_epsilon = variance_epsilon\n\n    def forward(self, x):\n        u = x.mean(-1, keepdim=True)\n        s = (x - u).pow(2).mean(-1, keepdim=True)\n        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n        return self.gamma * x + self.beta","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T02:37:32.654547Z","iopub.execute_input":"2025-05-06T02:37:32.654826Z","iopub.status.idle":"2025-05-06T02:37:35.786474Z","shell.execute_reply.started":"2025-05-06T02:37:32.654803Z","shell.execute_reply":"2025-05-06T02:37:35.785933Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# from vietocr.model.seqmodel.transformer import LanguageTransformer\nfrom torch import nn\nfrom transformers import ViTModel\n\nclass ViTEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n        self.projection = nn.Linear(768, config['transformer']['d_model'])\n        \n    def forward(self, x):\n        outputs = self.vit(x)\n        hidden_states = outputs.last_hidden_state  # (N, seq_len, 768)\n        return self.projection(hidden_states)      # (N, seq_len, d_model)\n\nclass VietOCR(nn.Module):\n    def __init__(\n        self,\n        vocab_size,  # Kept for compatibility but not used directly\n        backbone,    # Not used since we're replacing CNN with ViT\n        vit_args,    # Arguments for ViT configuration\n        transformer_args,\n        seq_modeling=\"transformer\",\n    ):\n        super(VietOCR, self).__init__()\n\n        # Replace CNN with ViTEncoder\n        self.encoder = ViTEncoder(vit_args)\n        self.seq_modeling = seq_modeling\n\n        if seq_modeling == \"transformer\":\n            # Remove explicit vocab_size, use transformer_args only\n            self.transformer = LanguageTransformer(**transformer_args)\n        else:\n            raise ValueError(\"Not Supported Seq Model\")\n\n    def forward(self, img, tgt_input, tgt_key_padding_mask):\n        \"\"\"\n        Shape:\n            - img: (N, C, H, W) - Input image tensor (e.g., 3 channels, 224x224 for ViT)\n            - tgt_input: (T, N) - Target input for the transformer decoder\n            - tgt_key_padding_mask: (N, T) - Padding mask for the target sequence\n            - output: (N, T, V) - Output logits over vocabulary\n        \"\"\"\n        # Encode image using ViT\n        src = self.encoder(img)  # Shape: (N, seq_len, d_model)\n\n        if self.seq_modeling == \"transformer\":\n            outputs = self.transformer(\n                src, tgt_input, tgt_key_padding_mask=tgt_key_padding_mask\n            )\n        else:\n            raise ValueError(\"Not Supported Seq Model\")\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T02:37:35.787129Z","iopub.execute_input":"2025-05-06T02:37:35.787401Z","iopub.status.idle":"2025-05-06T02:37:57.096215Z","shell.execute_reply.started":"2025-05-06T02:37:35.787384Z","shell.execute_reply":"2025-05-06T02:37:57.095391Z"}},"outputs":[{"name":"stderr","text":"2025-05-06 02:37:45.511466: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746499065.733918      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746499065.795449      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# 2. Vocab","metadata":{}},{"cell_type":"code","source":"class Vocab:\n    def __init__(self, chars):\n        self.pad = 0\n        self.go = 1\n        self.eos = 2\n        self.unk = 3\n        self.mask = 4\n        self.char2id = {'<pad>': 0, '<go>': 1, '<eos>': 2, '<unk>': 3, '<mask>': 4}\n        self.id2char = {0: '<pad>', 1: '<go>', 2: '<eos>', 3: '<unk>', 4: '<mask>'}\n        \n        for i, c in enumerate(sorted(chars), start=5):\n            self.char2id[c] = i\n            self.id2char[i] = c\n    \n    def encode(self, text):\n        \"\"\"Convert text to list of indices with <go> and <eos> tokens.\"\"\"\n        indices = [self.char2id['<go>']]\n        for char in text:\n            indices.append(self.char2id.get(char, self.char2id['<unk>']))\n        indices.append(self.char2id['<eos>'])\n        return torch.tensor(indices, dtype=torch.long)\n    \n    def decode(self, indices):\n        \"\"\"Convert list of indices to text.\"\"\"\n        text = []\n        for idx in indices:\n            # Handle both tensor and integer inputs\n            idx_val = idx.item() if hasattr(idx, 'item') else idx\n            if idx_val == self.char2id['<eos>']:\n                break\n            if idx_val not in (self.char2id['<pad>'], self.char2id['<go>'], self.char2id['<mask>']):\n                text.append(self.id2char.get(idx_val, '<unk>'))\n        return ''.join(text)\n\n    def __len__(self):\n        return len(self.char2id)\n    \n    def batch_decode(self, arr):\n        \"\"\"Decode a batch of index sequences.\"\"\"\n        texts = [self.decode(ids) for ids in arr]\n        return texts\n\n# Create vocabulary\ndef create_vocab(labels_file):\n    with open(labels_file, 'r', encoding='utf-8') as f:\n        labels = json.load(f)\n    \n    chars = set()\n    for label in labels.values():\n        chars.update(label)\n    \n    return Vocab(chars)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T02:37:57.098014Z","iopub.execute_input":"2025-05-06T02:37:57.098503Z","iopub.status.idle":"2025-05-06T02:37:57.106590Z","shell.execute_reply.started":"2025-05-06T02:37:57.098484Z","shell.execute_reply":"2025-05-06T02:37:57.105855Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# 3. Dataloader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader  # Import Dataset\n# Dataset\nclass CustomDataset(Dataset):\n    def __init__(self, data_dir, labels_file, vocab, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.vocab = vocab\n        \n        with open(labels_file, 'r', encoding='utf-8') as f:\n            self.labels = json.load(f)\n        \n        self.image_files = list(self.labels.keys())\n        \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n        img_path = os.path.join(self.data_dir, img_name)\n        \n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n            \n        label = self.labels[img_name]\n        label_indices = self.vocab.encode(label)\n        \n        return {\n            'img': image,\n            'label': label_indices,\n            'text': label,\n            'filename': img_name\n        }\n\ndef collate_fn(batch):\n    images = torch.stack([item['img'] for item in batch])\n    labels = [item['label'] for item in batch]\n    filenames = [item['filename'] for item in batch]\n    \n    max_len = max(len(label) for label in labels)\n    padded_labels = torch.zeros(len(labels), max_len, dtype=torch.long)\n    padding_mask = torch.ones(len(labels), max_len, dtype=torch.bool)\n    \n    for i, label in enumerate(labels):\n        padded_labels[i, :len(label)] = label\n        padding_mask[i, :len(label)] = False\n    \n    return {\n        'img': images,\n        'label': padded_labels,\n        'padding_mask': padding_mask,\n        'filenames': filenames\n    }","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T02:37:57.107358Z","iopub.execute_input":"2025-05-06T02:37:57.107638Z","iopub.status.idle":"2025-05-06T02:37:57.127040Z","shell.execute_reply.started":"2025-05-06T02:37:57.107620Z","shell.execute_reply":"2025-05-06T02:37:57.126413Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# 4. Loss","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\n\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, padding_idx, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n        self.padding_idx = padding_idx\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            # true_dist = pred.data.clone()\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 2))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n            true_dist[:, self.padding_idx] = 0\n            mask = torch.nonzero(target.data == self.padding_idx, as_tuple=False)\n            if mask.dim() > 0:\n                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T02:37:57.127675Z","iopub.execute_input":"2025-05-06T02:37:57.127876Z","iopub.status.idle":"2025-05-06T02:37:57.142327Z","shell.execute_reply.started":"2025-05-06T02:37:57.127860Z","shell.execute_reply":"2025-05-06T02:37:57.141609Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# 5. Beam","metadata":{}},{"cell_type":"code","source":"import torch\n\nclass Beam:\n    def __init__(self, beam_size, candidates, vocab_size, device, sos_token, eos_token):\n        self.beam_size = beam_size\n        self.candidates = candidates\n        self.vocab_size = vocab_size\n        self.device = device\n        self.sos_token = sos_token\n        self.eos_token = eos_token\n\n        # Initialize hypotheses: list of (sequence, score)\n        self.hypotheses = [[torch.tensor([sos_token], device=device), 0.0]]  # Start with SOS token\n        self.completed = []  # Store completed hypotheses (sequences that hit EOS)\n\n    def get_current_state(self):\n        \"\"\"Return current hypotheses as a tensor of shape (beam_size, seq_len).\"\"\"\n        if not self.hypotheses:\n            return torch.tensor([], device=self.device).long()\n\n        # Get max sequence length among current hypotheses\n        max_len = max(len(hyp[0]) for hyp in self.hypotheses)\n        # Pad sequences to max length\n        padded = torch.ones(len(self.hypotheses), max_len, device=self.device).long() * self.eos_token\n        for i, (seq, _) in enumerate(self.hypotheses):\n            padded[i, :len(seq)] = seq\n        return padded  # Shape: (beam_size, seq_len)\n\n    def advance(self, log_probs):\n        \"\"\"Update beam with new log probabilities.\n        log_probs: Shape (beam_size, vocab_size)\n        \"\"\"\n        if not self.hypotheses:\n            return\n\n        # Get current scores\n        scores = torch.tensor([score for _, score in self.hypotheses], device=self.device)\n        # Compute scores for all possible next tokens\n        # Shape: (beam_size, vocab_size)\n        candidate_scores = scores.unsqueeze(1) + log_probs\n\n        # Flatten to get top-k scores across all candidates\n        flat_scores = candidate_scores.view(-1)  # Shape: (beam_size * vocab_size)\n        topk_scores, topk_indices = flat_scores.topk(min(self.beam_size, len(flat_scores)), dim=0)\n\n        # Compute which hypothesis and token each top-k score corresponds to\n        new_hypotheses = []\n        for score, idx in zip(topk_scores, topk_indices):\n            prev_hyp_idx = idx // self.vocab_size\n            token = idx % self.vocab_size\n            prev_seq, prev_score = self.hypotheses[prev_hyp_idx]\n            new_seq = torch.cat([prev_seq, torch.tensor([token], device=self.device)])\n\n            if token == self.eos_token:\n                # If EOS, add to completed hypotheses\n                self.completed.append([new_seq, score.item()])\n            else:\n                # Otherwise, add to active hypotheses\n                new_hypotheses.append([new_seq, score.item()])\n\n        # Keep only top beam_size hypotheses\n        self.hypotheses = new_hypotheses[:self.beam_size]\n\n        # If no active hypotheses remain, stop\n        if not self.hypotheses:\n            self.hypotheses = [[torch.tensor([self.sos_token], device=self.device), 0.0]]\n\n    def all_eos(self):\n        \"\"\"Check if all active hypotheses have reached EOS.\"\"\"\n        if not self.hypotheses:\n            return True\n        return all(seq[-1] == self.eos_token for seq, _ in self.hypotheses)\n\n    def get_final(self, candidates):\n        \"\"\"Return the top candidates completed hypotheses.\"\"\"\n        # Sort completed hypotheses by score\n        self.completed.sort(key=lambda x: x[1], reverse=True)\n        # Take top candidates\n        top_completed = self.completed[:candidates]\n        if not top_completed:\n            # If no completed hypotheses, return best active hypothesis\n            if self.hypotheses:\n                top_completed = [self.hypotheses[0]]\n            else:\n                top_completed = [[torch.tensor([self.sos_token], device=self.device), 0.0]]\n\n        # Convert sequences to list of integers and return with scores\n        final_sents = [seq.tolist() for seq, _ in top_completed]\n        final_probs = [score for _, score in top_completed]\n        return final_sents, final_probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T02:37:57.143036Z","iopub.execute_input":"2025-05-06T02:37:57.143239Z","iopub.status.idle":"2025-05-06T02:37:57.163508Z","shell.execute_reply.started":"2025-05-06T02:37:57.143224Z","shell.execute_reply":"2025-05-06T02:37:57.162627Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport math\nfrom PIL import Image\nfrom torch.nn.functional import log_softmax, softmax\n\n\ndef batch_translate_beam_search(img, model, beam_size=4, candidates=1, max_seq_length=128, sos_token=1, eos_token=2):\n    model.eval()\n    device = img.device\n    sents = []\n\n    with torch.no_grad():\n        src = model.encoder(img)  # Shape: (batch_size, seq_len, d_model)\n        memories = model.transformer.forward_encoder(src)  # Shape: (batch_size, seq_len, d_model)\n        batch_size = memories.size(0)\n\n        for i in range(batch_size):\n            memory = model.transformer.get_memory(memories, i)  # Shape: (1, seq_len, d_model)\n            memory = model.transformer.expand_memory(memory, beam_size)  # Shape: (beam_size, seq_len, d_model)\n            sent = beamsearch(\n                memory, model, device, beam_size, candidates, max_seq_length, sos_token, eos_token\n            )  # Shape: (candidates, max_seq_length)\n            # assert sent.dim() == 2 and sent.size(0) == candidates and sent.size(1) == max_seq_length, \\\n            #     f\"Expected shape ({candidates}, {max_seq_length}), got {sent.shape}\"\n            sents.append(sent)\n\n        # Concatenate all sequences (all are already padded to max_seq_length)\n        sents = torch.cat(sents, dim=0)  # Shape: (batch_size * candidates, max_seq_length)\n    return sents\n\n\ndef beamsearch(memory, model, device, beam_size=4, candidates=1, max_seq_length=128, sos_token=1, eos_token=2):\n    final_sents = []\n    final_probs = []\n\n    # Initial input: <sos> token\n    tgt_inp = torch.LongTensor(beam_size, 1).fill_(sos_token).to(device)  # Shape: (beam_size, 1)\n\n    # Initial log probabilities\n    log_probs = torch.zeros(beam_size, 1).to(device)  # Shape: (beam_size, 1)\n\n    # Track whether each beam is finished\n    finished = torch.zeros(beam_size, dtype=torch.bool).to(device)\n    memory = memory.to(device)  # Ensure memory is on the correct device\n\n    for _ in range(max_seq_length):\n        # Forward pass through decoder\n        decoder_outputs, memory = model.transformer.forward_decoder(tgt_inp, memory)\n\n        # Get log probabilities for the last token\n        log_prob = torch.log_softmax(decoder_outputs[:, -1, :], dim=-1)  # Shape: (beam_size, vocab_size)\n\n        # Accumulate log probabilities\n        log_probs = log_probs + log_prob  # Shape: (beam_size, vocab_size)\n\n        # Flatten for top-k selection\n        log_probs_flat = log_probs.view(-1)  # Shape: (beam_size * vocab_size)\n        top_v, top_i = log_probs_flat.topk(beam_size, dim=0)\n\n        # Compute new beam indices and token indices\n        beam_indices = top_i // log_prob.size(-1)  # Shape: (beam_size)\n        token_indices = top_i % log_prob.size(-1)  # Shape: (beam_size)\n\n        # Update log_probs and tgt_inp\n        new_log_probs = torch.zeros(beam_size, 1).to(device)\n        new_tgt_inp = torch.zeros(beam_size, tgt_inp.size(1) + 1, dtype=torch.long).to(device)\n\n        for j in range(beam_size):\n            if finished[j]:\n                new_log_probs[j] = log_probs[j].sum().unsqueeze(0)\n                new_tgt_inp[j, :-1] = tgt_inp[j]\n                new_tgt_inp[j, -1] = eos_token\n                continue\n\n            beam_idx = beam_indices[j]\n            token_idx = token_indices[j]\n            new_log_probs[j] = top_v[j].unsqueeze(0)\n            new_tgt_inp[j, :-1] = tgt_inp[beam_idx]\n            new_tgt_inp[j, -1] = token_idx\n\n            # Check if the token is <eos>\n            if token_idx == eos_token:\n                finished[j] = True\n                # Pad the sequence to max_seq_length before appending\n                seq_len = new_tgt_inp[j:j+1].size(1)\n                if seq_len < max_seq_length:\n                    padding = torch.full((1, max_seq_length - seq_len), eos_token, dtype=torch.long, device=device)\n                    padded_sent = torch.cat([new_tgt_inp[j:j+1], padding], dim=1)\n                else:\n                    padded_sent = new_tgt_inp[j:j+1]\n                final_sents.append(padded_sent)\n                final_probs.append(new_log_probs[j].item())  # Append scalar probability\n\n        log_probs = new_log_probs\n        tgt_inp = new_tgt_inp\n\n        # If all beams are finished, break\n        if finished.all():\n            break\n\n    # If no sentences finished, take the best and pad it\n    if not final_sents:\n        seq_len = tgt_inp[0:1].size(1)\n        if seq_len < max_seq_length:\n            padding = torch.full((1, max_seq_length - seq_len), eos_token, dtype=torch.long, device=device)\n            padded_sent = torch.cat([tgt_inp[0:1], padding], dim=1)\n        else:\n            padded_sent = tgt_inp[0:1]\n        final_sents.append(padded_sent)\n        final_probs.append(log_probs[0].sum().item())  # Append scalar probability\n\n    # Concatenate final sentences and probabilities\n    final_sents = torch.cat(final_sents, dim=0)  # Shape: (num_finished_beams, max_seq_length)\n    final_probs = torch.tensor(final_probs, device=device)  # Shape: (num_finished_beams,)\n\n    # Select the best candidates\n    _, idx = final_probs.topk(min(candidates, final_probs.size(0)), dim=0)  # Shape: (candidates,)\n    idx = idx.squeeze()  # Ensure idx is 1D\n    if idx.dim() == 0:  # Handle case where idx is a scalar (e.g., candidates=1)\n        idx = idx.unsqueeze(0)\n    return final_sents[idx]  # Shape: (candidates, max_seq_length)\ndef translate_beam_search(\n    img, model, beam_size=4, candidates=1, max_seq_length=128, sos_token=1, eos_token=2\n):\n    model.eval()\n    device = img.device\n\n    with torch.no_grad():\n        src = model.encoder(img)  # Shape: (1, seq_len, d_model)\n        memory = model.transformer.forward_encoder(src)  # Shape: (1, seq_len, d_model)\n        sent = beamsearch(\n            memory, model, device, beam_size, candidates, max_seq_length, sos_token, eos_token\n        )  # Shape: (candidates, max_seq_length)\n\n    return sent\n\n\ndef translate(img, model, max_seq_length=128, sos_token=1, eos_token=2):\n    model.eval()\n    device = img.device\n\n    with torch.no_grad():\n        src = model.encoder(img)  # Shape: (batch_size, seq_len, d_model)\n        memory = model.transformer.forward_encoder(src)  # Shape: (batch_size, seq_len, d_model)\n\n        translated_sentence = [[sos_token] * len(img)]\n        char_probs = [[1] * len(img)]\n\n        max_length = 0\n\n        while max_length <= max_seq_length and not all(\n            np.any(np.asarray(translated_sentence).T == eos_token, axis=1)\n        ):\n            tgt_inp = torch.LongTensor(translated_sentence).to(device)\n            output, memory = model.transformer.forward_decoder(tgt_inp, memory)\n            output = softmax(output, dim=-1)\n            output = output.to(\"cpu\")\n\n            values, indices = torch.topk(output, 5)\n            indices = indices[:, -1, 0]\n            indices = indices.tolist()\n            values = values[:, -1, 0]\n            values = values.tolist()\n            char_probs.append(values)\n\n            translated_sentence.append(indices)\n            max_length += 1\n\n            del output\n\n        translated_sentence = np.asarray(translated_sentence).T\n        char_probs = np.asarray(char_probs).T\n        char_probs = np.multiply(char_probs, translated_sentence > 3)\n        char_probs = np.sum(char_probs, axis=-1) / (char_probs > 0).sum(-1)\n\n    return translated_sentence, char_probs\n\n\ndef build_model(config):\n    vocab = Vocab(config[\"vocab\"])\n    device = config[\"device\"]\n\n    model = VietOCR(\n        len(vocab),\n        config[\"backbone\"],\n        config[\"cnn\"],\n        config[\"transformer\"],\n        config[\"seq_modeling\"],\n    )\n\n    model = model.to(device)\n    return model, vocab\n\n\ndef resize(w, h, expected_height, image_min_width, image_max_width):\n    new_w = int(expected_height * float(w) / float(h))\n    round_to = 10\n    new_w = math.ceil(new_w / round_to) * round_to\n    new_w = max(new_w, image_min_width)\n    new_w = min(new_w, image_max_width)\n    return new_w, expected_height\n\n\ndef process_image(image, image_height, image_min_width, image_max_width):\n    img = image.convert(\"RGB\")\n    w, h = img.size\n    new_w, image_height = resize(w, h, image_height, image_min_width, image_max_width)\n    img = img.resize((new_w, image_height), Image.LANCZOS)\n    img = np.asarray(img).transpose(2, 0, 1)\n    img = img / 255\n    return img\n\n\ndef process_input(image, image_height, image_min_width, image_max_width):\n    img = process_image(image, image_height, image_min_width, image_max_width)\n    img = img[np.newaxis, ...]\n    img = torch.FloatTensor(img)\n    return img\n\n\ndef predict(filename, config):\n    img = Image.open(filename)\n    img = process_input(img, config[\"image_height\"], config[\"image_min_width\"], config[\"image_max_width\"])\n    img = img.to(config[\"device\"])\n    model, vocab = build_model(config)\n    s = translate(img, model)[0].tolist()\n    s = vocab.decode(s)\n    return s","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:18:14.143724Z","iopub.execute_input":"2025-05-06T04:18:14.144362Z","iopub.status.idle":"2025-05-06T04:18:14.166971Z","shell.execute_reply.started":"2025-05-06T04:18:14.144334Z","shell.execute_reply":"2025-05-06T04:18:14.166369Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# 6. Metrics","metadata":{}},{"cell_type":"code","source":"import os\nimport gdown\nimport yaml\nimport numpy as np\nimport uuid\nimport requests\nimport tempfile\nfrom tqdm import tqdm\n\n\ndef download_weights(uri, cached=None, md5=None, quiet=False):\n    if uri.startswith(\"http\"):\n        return download(url=uri, quiet=quiet)\n    return uri\n\n\ndef download(url, quiet=False):\n    tmp_dir = tempfile.gettempdir()\n    filename = url.split(\"/\")[-1]\n    full_path = os.path.join(tmp_dir, filename)\n\n    if os.path.exists(full_path):\n        print(\"Model weight {} exsits. Ignore download!\".format(full_path))\n        return full_path\n\n    with requests.get(url, stream=True) as r:\n        r.raise_for_status()\n        with open(full_path, \"wb\") as f:\n            for chunk in tqdm(r.iter_content(chunk_size=8192)):\n                # If you have chunk encoded response uncomment if\n                # and set chunk_size parameter to None.\n                # if chunk:\n                f.write(chunk)\n    return full_path\n\n\ndef download_config(id):\n    url = \"https://vocr.vn/data/vietocr/config/{}\".format(id)\n    r = requests.get(url)\n    config = yaml.safe_load(r.text)\n    return config\n\n\ndef compute_accuracy(ground_truth, predictions, mode=\"full_sequence\"):\n    \"\"\"\n    Computes accuracy\n    :param ground_truth:\n    :param predictions:\n    :param display: Whether to print values to stdout\n    :param mode: if 'per_char' is selected then\n                 single_label_accuracy = correct_predicted_char_nums_of_single_sample / single_label_char_nums\n                 avg_label_accuracy = sum(single_label_accuracy) / label_nums\n                 if 'full_sequence' is selected then\n                 single_label_accuracy = 1 if the prediction result is exactly the same as label else 0\n                 avg_label_accuracy = sum(single_label_accuracy) / label_nums\n    :return: avg_label_accuracy\n    \"\"\"\n    if mode == \"per_char\":\n\n        accuracy = []\n\n        for index, label in enumerate(ground_truth):\n            prediction = predictions[index]\n            total_count = len(label)\n            correct_count = 0\n            try:\n                for i, tmp in enumerate(label):\n                    if tmp == prediction[i]:\n                        correct_count += 1\n            except IndexError:\n                continue\n            finally:\n                try:\n                    accuracy.append(correct_count / total_count)\n                except ZeroDivisionError:\n                    if len(prediction) == 0:\n                        accuracy.append(1)\n                    else:\n                        accuracy.append(0)\n        avg_accuracy = np.mean(np.array(accuracy).astype(np.float32), axis=0)\n    elif mode == \"full_sequence\":\n        try:\n            correct_count = 0\n            for index, label in enumerate(ground_truth):\n                prediction = predictions[index]\n                if prediction == label:\n                    correct_count += 1\n            avg_accuracy = correct_count / len(ground_truth)\n        except ZeroDivisionError:\n            if not predictions:\n                avg_accuracy = 1\n            else:\n                avg_accuracy = 0\n    else:\n        raise NotImplementedError(\n            \"Other accuracy compute mode has not been implemented\"\n        )\n\n    return avg_accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T02:37:57.204165Z","iopub.execute_input":"2025-05-06T02:37:57.204585Z","iopub.status.idle":"2025-05-06T02:37:57.518351Z","shell.execute_reply.started":"2025-05-06T02:37:57.204544Z","shell.execute_reply":"2025-05-06T02:37:57.517821Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# 7. Logger","metadata":{}},{"cell_type":"code","source":"import os\n\n\nclass Logger:\n    def __init__(self, fname):\n        path, _ = os.path.split(fname)\n        os.makedirs(path, exist_ok=True)\n\n        self.logger = open(fname, \"w\")\n\n    def log(self, string):\n        self.logger.write(string + \"\\n\")\n        self.logger.flush()\n\n    def close(self):\n        self.logger.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T02:37:57.520453Z","iopub.execute_input":"2025-05-06T02:37:57.520896Z","iopub.status.idle":"2025-05-06T02:37:57.525061Z","shell.execute_reply.started":"2025-05-06T02:37:57.520879Z","shell.execute_reply":"2025-05-06T02:37:57.524480Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# 8. Trainer","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport os\nfrom tqdm import tqdm\n\n\nclass Trainer:\n    def __init__(self, config, model, vocab, train_dataset, valid_dataset=None, pretrained=True):\n        self.config = config\n        self.model = model\n        self.vocab = vocab\n        self.device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n        self.model.to(self.device)\n        self.num_iters = config['train']['max_iters']\n        self.beamsearch = config['predictor']['beamsearch']\n\n        self.batch_size = config['train']['batch_size']\n        self.print_every = config['train']['print_every']\n        self.valid_every = config['train']['valid_every']\n        self.checkpoint_path = config['train']['checkpoint']\n        self.export_weights = config['train']['export']\n        self.metrics = None\n        logger = config['train'].get('log', None)\n\n        if logger:\n            self.logger = Logger(logger)\n\n        if pretrained and config.get('pretrain'):\n            self.load_weights(config['pretrain'])\n\n        self.optimizer = optim.AdamW(self.model.parameters(), betas=(0.9, 0.98), eps=1e-09)\n        self.scheduler = OneCycleLR(\n            self.optimizer, total_steps=self.num_iters, **config['optimizer']\n        )\n        self.criterion = LabelSmoothingLoss(\n            len(self.vocab), padding_idx=self.vocab.pad, smoothing=0.1\n        )\n\n        # Create data loaders\n        self.train_gen = DataLoader(\n            train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=config['dataloader']['num_workers'],\n            pin_memory=config['dataloader']['pin_memory'],\n            collate_fn=collate_fn\n        )\n        if valid_dataset:\n            self.valid_gen = DataLoader(\n                valid_dataset,\n                batch_size=self.batch_size,\n                shuffle=False,\n                num_workers=config['dataloader']['num_workers'],\n                pin_memory=config['dataloader']['pin_memory'],\n                collate_fn=collate_fn\n            )\n\n        self.iter = 0\n        self.train_losses = []\n\n    def train(self):\n        total_loss = 0\n        total_loader_time = 0\n        total_gpu_time = 0\n        best_acc = 0\n\n        data_iter = iter(self.train_gen)\n        for i in tqdm(range(self.num_iters)):\n            self.iter += 1\n            start = time.time()\n\n            try:\n                batch = next(data_iter)\n            except StopIteration:\n                data_iter = iter(self.train_gen)\n                batch = next(data_iter)\n\n            total_loader_time += time.time() - start\n            start = time.time()\n            loss = self.step(batch)\n            total_gpu_time += time.time() - start\n\n            total_loss += loss\n            self.train_losses.append((self.iter, loss))\n\n            if self.iter % self.print_every == 0:\n                info = \"iter: {:06d} - train loss: {:.3f} - lr: {:.2e} - load time: {:.2f} - gpu time: {:.2f}\".format(\n                    self.iter,\n                    total_loss / self.print_every,\n                    self.optimizer.param_groups[0][\"lr\"],\n                    total_loader_time,\n                    total_gpu_time,\n                )\n                print(info)\n                if hasattr(self, 'logger'):\n                    self.logger.log(info)\n                total_loss = 0\n                total_loader_time = 0\n                total_gpu_time = 0\n\n            if self.valid_gen and self.iter % self.valid_every == 0:\n                val_loss = self.validate()\n                acc_full_seq, acc_per_char = self.precision(self.metrics)\n                info = \"iter: {:06d} - valid loss: {:.3f} - acc full seq: {:.4f} - acc per char: {:.4f}\".format(\n                    self.iter, val_loss, acc_full_seq, acc_per_char\n                )\n                print(info)\n                if hasattr(self, 'logger'):\n                    self.logger.log(info)\n\n                if acc_full_seq > best_acc:\n                    self.save_weights(self.export_weights)\n                    best_acc = acc_full_seq\n\n        # Save final model\n        self.save_weights(self.export_weights)\n        print(f'Saved final model to {self.export_weights}')\n\n    def step(self, batch):\n        self.model.train()\n        batch = self.batch_to_device(batch)\n        img, tgt_input, tgt_output, tgt_padding_mask = (\n            batch['img'],\n            batch['label'][:, :-1],  # Exclude <eos>\n            batch['label'][:, 1:],   # Exclude <go>\n            batch['padding_mask'][:, :-1]\n        )\n    \n        outputs = self.model(img, tgt_input, tgt_key_padding_mask=tgt_padding_mask)\n        outputs = outputs.reshape(-1, outputs.size(2))  # Flatten (N, T, V) to (N*T, V)\n        tgt_output = tgt_output.reshape(-1)  # Flatten (N, T) to (N*T)\n    \n        loss = self.criterion(outputs, tgt_output)\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1)\n        self.optimizer.step()\n        self.scheduler.step()\n    \n        return loss.item()\n\n    def validate(self):\n        self.model.eval()\n        total_loss = []\n    \n        with torch.no_grad():\n            for step, batch in enumerate(self.valid_gen):\n                batch = self.batch_to_device(batch)\n    \n                img = batch['img']\n                tgt_input = batch['label'][:, :-1]\n                tgt_output = batch['label'][:, 1:]\n                tgt_padding_mask = batch['padding_mask'][:, :-1]\n    \n                outputs = self.model(img, tgt_input, tgt_key_padding_mask=tgt_padding_mask)\n                outputs = outputs.flatten(0, 1)\n                tgt_output = tgt_output.flatten()\n                loss = self.criterion(outputs, tgt_output)\n    \n                total_loss.append(loss.item())\n                del outputs\n                del loss\n    \n        total_loss = np.mean(total_loss)\n        self.model.train()\n        return total_loss\n\n    def predict(self, sample=None):\n        pred_sents = []\n        actual_sents = []\n        img_files = []\n\n        self.model.eval()\n        with torch.no_grad():\n            for batch in self.valid_gen:\n                batch = self.batch_to_device(batch)\n                if self.beamsearch:\n                    translated_sentence = batch_translate_beam_search(batch['img'], self.model)\n                    prob = None\n                else:\n                    translated_sentence, prob = translate(batch['img'], self.model)\n\n                pred_sent = self.vocab.batch_decode(translated_sentence.tolist())\n                actual_sent = self.vocab.batch_decode(batch['label'].tolist())\n                img_files.extend([os.path.basename(f) for f in batch['filenames']])\n\n                pred_sents.extend(pred_sent)\n                actual_sents.extend(actual_sent)\n\n                if sample and len(pred_sents) > sample:\n                    break\n\n        return pred_sents, actual_sents, img_files, prob\n\n    def precision(self, sample=None):\n        pred_sents, actual_sents, _, _ = self.predict(sample=sample)\n        acc_full_seq = compute_accuracy(actual_sents, pred_sents, mode=\"full_sequence\")\n        acc_per_char = compute_accuracy(actual_sents, pred_sents, mode=\"per_char\")\n        return acc_full_seq, acc_per_char\n\n    def visualize_prediction(self, sample=16, errorcase=False, fontname=\"serif\", fontsize=16):\n        pred_sents, actual_sents, img_files, probs = self.predict(sample)\n        data_dir = self.config['dataset']['data_root']\n\n        if errorcase:\n            wrongs = [i for i in range(len(pred_sents)) if pred_sents[i] != actual_sents[i]]\n            pred_sents = [pred_sents[i] for i in wrongs]\n            actual_sents = [actual_sents[i] for i in wrongs]\n            img_files = [img_files[i] for i in wrongs]\n            probs = [probs[i] for i in wrongs] if probs else None\n\n        img_files = img_files[:sample]\n        fontdict = {\"family\": fontname, \"size\": fontsize}\n\n        for vis_idx, img_file in enumerate(img_files):\n            img_path = os.path.join(data_dir, img_file)\n            pred_sent = pred_sents[vis_idx]\n            actual_sent = actual_sents[vis_idx]\n            prob = probs[vis_idx] if probs else None\n\n            img = Image.open(img_path)\n            plt.figure()\n            plt.imshow(img)\n            title = f\"pred: {pred_sent} - actual: {actual_sent}\"\n            if prob is not None:\n                title = f\"prob: {prob:.3f} - {title}\"\n            plt.title(title, loc=\"left\", fontdict=fontdict)\n            plt.axis(\"off\")\n            plt.show()\n\n    def batch_to_device(self, batch):\n        return {\n            'img': batch['img'].to(self.device, non_blocking=True),\n            'label': batch['label'].to(self.device, non_blocking=True),\n            'padding_mask': batch['padding_mask'].to(self.device, non_blocking=True),\n            'filenames': batch['filenames']\n        }\n\n    def load_weights(self, filename):\n        state_dict = torch.load(filename, map_location=self.device)\n        self.model.load_state_dict(state_dict, strict=False)\n\n    def save_weights(self, filename):\n        os.makedirs(os.path.dirname(filename), exist_ok=True)\n        torch.save(self.model.state_dict(), filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T02:37:57.525900Z","iopub.execute_input":"2025-05-06T02:37:57.526141Z","iopub.status.idle":"2025-05-06T02:37:57.553391Z","shell.execute_reply.started":"2025-05-06T02:37:57.526126Z","shell.execute_reply":"2025-05-06T02:37:57.552787Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# 9. Train","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nfrom sklearn.model_selection import train_test_split\n\ndef split_dataset(labels_file, output_dir, train_ratio=0.8, random_seed=42):\n    \"\"\"\n    Split the labels.json file into training and validation sets.\n    \n    Args:\n        labels_file (str): Path to labels.json\n        output_dir (str): Directory to save train_labels.json and valid_labels.json\n        train_ratio (float): Proportion of data for training (default: 0.8)\n        random_seed (int): Seed for reproducibility\n    \"\"\"\n    # Load the labels\n    with open(labels_file, 'r', encoding='utf-8') as f:\n        labels = json.load(f)\n    \n    # Get list of image filenames and corresponding labels\n    image_files = list(labels.keys())\n    label_texts = list(labels.values())\n    \n    # Optional: Stratify by label length for balanced splits\n    # Create bins based on text length\n    lengths = [len(text) for text in label_texts]\n    bins = [min(l, 20) for l in lengths]  # Cap at 20 for reasonable binning\n    \n    # Split the data\n    train_files, valid_files, train_labels, valid_labels = train_test_split(\n        image_files,\n        label_texts,\n        train_size=train_ratio,\n        random_state=random_seed,\n        stratify=bins  # Stratify by text length\n    )\n    \n    # Create dictionaries for train and valid sets\n    train_dict = dict(zip(train_files, train_labels))\n    valid_dict = dict(zip(valid_files, valid_labels))\n    \n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Save train and valid JSON files\n    train_json = os.path.join(output_dir, 'train_labels.json')\n    valid_json = os.path.join(output_dir, 'valid_labels.json')\n    \n    with open(train_json, 'w', encoding='utf-8') as f:\n        json.dump(train_dict, f, ensure_ascii=False, indent=2)\n    \n    with open(valid_json, 'w', encoding='utf-8') as f:\n        json.dump(valid_dict, f, ensure_ascii=False, indent=2)\n    \n    print(f\"Saved training labels to {train_json} ({len(train_dict)} samples)\")\n    print(f\"Saved validation labels to {valid_json} ({len(valid_dict)} samples)\")\n    \n    return train_json, valid_json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T02:37:57.554114Z","iopub.execute_input":"2025-05-06T02:37:57.554459Z","iopub.status.idle":"2025-05-06T02:37:57.581948Z","shell.execute_reply.started":"2025-05-06T02:37:57.554436Z","shell.execute_reply":"2025-05-06T02:37:57.581379Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\n\n# Configuration\ndef create_vit_config():\n    config = {\n        'transformer': {\n            'vocab_size': None,  # Will be set after vocab creation\n            'd_model': 256,\n            'nhead': 8,\n            'num_encoder_layers': 6,\n            'num_decoder_layers': 6,\n            'dim_feedforward': 2048,\n            'max_seq_length': 1024,  # Increased to match ViT sequence length\n            'pos_dropout': 0.1,\n            'trans_dropout': 0.1\n        },\n        'train': {\n            'batch_size': 8,\n            'epochs': 100,\n            'print_every': 200,\n            'valid_every': 1000,\n            'checkpoint': './checkpoint/vit_transformer_checkpoint.pth',\n            'export': './weights/vit_transformer.pth',\n            'max_iters': 10000,\n            'learning_rate': 1e-4\n        }\n    }\n    return config\n\n# Function to split dataset (from Step 1)\ndef split_dataset(labels_file, output_dir, train_ratio=0.8, random_seed=42):\n    with open(labels_file, 'r', encoding='utf-8') as f:\n        labels = json.load(f)\n    \n    image_files = list(labels.keys())\n    label_texts = list(labels.values())\n    lengths = [len(text) for text in label_texts]\n    bins = [min(l, 20) for l in lengths]\n    \n    train_files, valid_files, train_labels, valid_labels = train_test_split(\n        image_files,\n        label_texts,\n        train_size=train_ratio,\n        random_state=random_seed,\n        stratify=bins\n    )\n    \n    train_dict = dict(zip(train_files, train_labels))\n    valid_dict = dict(zip(valid_files, valid_labels))\n    \n    os.makedirs(output_dir, exist_ok=True)\n    train_json = os.path.join(output_dir, 'train_labels.json')\n    valid_json = os.path.join(output_dir, 'valid_labels.json')\n    \n    with open(train_json, 'w', encoding='utf-8') as f:\n        json.dump(train_dict, f, ensure_ascii=False, indent=2)\n    with open(valid_json, 'w', encoding='utf-8') as f:\n        json.dump(valid_dict, f, ensure_ascii=False, indent=2)\n    \n    print(f\"Saved training labels to {train_json} ({len(train_dict)} samples)\")\n    print(f\"Saved validation labels to {valid_json} ({len(valid_dict)} samples)\")\n    \n    return train_json, valid_json\n\n# Main training function\ndef train():\n    # Configuration\n    config = create_vit_config()\n    config.update({\n        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n        'dataset': {\n            'data_root': '/kaggle/input/dataset-ocr/dataset/data',\n            'train_annotation': None,  # Will be set after splitting\n            'valid_annotation': None,  # Will be set after splitting\n            'name': 'ocr_dataset',\n            'image_height': 224,\n            'image_min_width': 224,\n            'image_max_width': 224\n        },\n        'aug': {\n            'image_aug': False,\n            'masked_language_model': False\n        },\n        'optimizer': {\n            'max_lr': 1e-4,\n            'pct_start': 0.1,\n            'anneal_strategy': 'cos'\n        },\n        'dataloader': {\n            'num_workers': 1,\n            'pin_memory': True\n        },\n        'predictor': {\n            'beamsearch': True\n        },\n        'pretrain': '/kaggle/input/vietocr/pytorch/default/1/vgg_transformer.pth',\n        'quiet': False\n    })\n\n    # Split the dataset\n    original_labels_file = '/kaggle/input/dataset-ocr/dataset/labels.json'\n    output_dir = './dataset_splits'\n    train_json, valid_json = split_dataset(original_labels_file, output_dir, train_ratio=0.8, random_seed=42)\n    \n    # Update config with new annotation files\n    config['dataset']['train_annotation'] = train_json\n    config['dataset']['valid_annotation'] = valid_json\n\n    # Create vocabulary\n    vocab = 'aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!\"#$%&''()*+,-./:;<=>?@[\\]^_`{|}~ '\n    vocab = Vocab(vocab)\n    config['transformer']['vocab_size'] = len(vocab)\n\n    # Define image transforms\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    # Create datasets\n    train_dataset = CustomDataset(\n        data_dir=config['dataset']['data_root'],\n        labels_file=config['dataset']['train_annotation'],\n        vocab=vocab,\n        transform=transform\n    )\n    valid_dataset = CustomDataset(\n        data_dir=config['dataset']['data_root'],\n        labels_file=config['dataset']['valid_annotation'],\n        vocab=vocab,\n        transform=transform\n    )\n\n    # Create model\n    model = VietOCR(\n        vocab_size=len(vocab),\n        backbone=None,\n        vit_args=config,\n        transformer_args=config['transformer'],\n        seq_modeling='transformer'\n    )\n\n    # Create trainer\n    trainer = Trainer(\n        config=config,\n        model=model,\n        vocab=vocab,\n        train_dataset=train_dataset,\n        valid_dataset=valid_dataset,\n        pretrained=True\n    )\n\n    # Train model\n    trainer.train()\n\nif __name__ == '__main__':\n    os.makedirs('./checkpoint', exist_ok=True)\n    os.makedirs('./weights', exist_ok=True)\n    os.makedirs('./dataset_splits', exist_ok=True)\n    train()","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-05-06T05:49:05.457837Z","shell.execute_reply.started":"2025-05-06T04:18:22.020731Z","shell.execute_reply":"2025-05-06T05:49:05.457014Z"}},"outputs":[{"name":"stderr","text":" 70%|███████   | 7000/10000 [1:03:46<39:50:07, 47.80s/it]","output_type":"stream"},{"name":"stdout","text":"iter: 007000 - valid loss: 1.595 - acc full seq: 0.0000 - acc per char: 0.0765\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 7200/10000 [1:05:03<17:49,  2.62it/s]   ","output_type":"stream"},{"name":"stdout","text":"iter: 007200 - train loss: 1.403 - lr: 2.20e-05 - load time: 0.34 - gpu time: 76.81\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▍  | 7400/10000 [1:06:20<16:36,  2.61it/s]","output_type":"stream"},{"name":"stdout","text":"iter: 007400 - train loss: 1.395 - lr: 1.92e-05 - load time: 0.32 - gpu time: 76.18\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▌  | 7600/10000 [1:07:37<15:17,  2.62it/s]","output_type":"stream"},{"name":"stdout","text":"iter: 007600 - train loss: 1.396 - lr: 1.65e-05 - load time: 0.33 - gpu time: 75.97\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 7800/10000 [1:08:53<14:00,  2.62it/s]","output_type":"stream"},{"name":"stdout","text":"iter: 007800 - train loss: 1.393 - lr: 1.40e-05 - load time: 0.36 - gpu time: 76.08\n","output_type":"stream"},{"name":"stderr","text":" 80%|███████▉  | 7999/10000 [1:10:09<12:46,  2.61it/s]","output_type":"stream"},{"name":"stdout","text":"iter: 008000 - train loss: 1.394 - lr: 1.17e-05 - load time: 0.32 - gpu time: 76.08\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 8000/10000 [1:12:42<25:40:45, 46.22s/it]","output_type":"stream"},{"name":"stdout","text":"iter: 008000 - valid loss: 1.578 - acc full seq: 0.0000 - acc per char: 0.0728\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▏ | 8200/10000 [1:13:59<11:25,  2.63it/s]   ","output_type":"stream"},{"name":"stdout","text":"iter: 008200 - train loss: 1.386 - lr: 9.54e-06 - load time: 0.32 - gpu time: 76.39\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▍ | 8400/10000 [1:15:16<10:13,  2.61it/s]","output_type":"stream"},{"name":"stdout","text":"iter: 008400 - train loss: 1.389 - lr: 7.59e-06 - load time: 0.33 - gpu time: 76.29\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▌ | 8600/10000 [1:16:33<08:54,  2.62it/s]","output_type":"stream"},{"name":"stdout","text":"iter: 008600 - train loss: 1.388 - lr: 5.84e-06 - load time: 0.31 - gpu time: 75.98\n","output_type":"stream"},{"name":"stderr","text":" 88%|████████▊ | 8800/10000 [1:17:49<07:40,  2.61it/s]","output_type":"stream"},{"name":"stdout","text":"iter: 008800 - train loss: 1.384 - lr: 4.32e-06 - load time: 0.32 - gpu time: 76.07\n","output_type":"stream"},{"name":"stderr","text":" 90%|████████▉ | 8999/10000 [1:19:05<06:20,  2.63it/s]","output_type":"stream"},{"name":"stdout","text":"iter: 009000 - train loss: 1.388 - lr: 3.01e-06 - load time: 0.33 - gpu time: 76.05\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 9000/10000 [1:21:42<13:09:25, 47.37s/it]","output_type":"stream"},{"name":"stdout","text":"iter: 009000 - valid loss: 1.587 - acc full seq: 0.0000 - acc per char: 0.0754\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 9200/10000 [1:22:59<04:47,  2.78it/s]   ","output_type":"stream"},{"name":"stdout","text":"iter: 009200 - train loss: 1.390 - lr: 1.93e-06 - load time: 0.32 - gpu time: 76.32\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▍| 9400/10000 [1:24:16<03:48,  2.63it/s]","output_type":"stream"},{"name":"stdout","text":"iter: 009400 - train loss: 1.384 - lr: 1.09e-06 - load time: 0.54 - gpu time: 76.07\n","output_type":"stream"},{"name":"stderr","text":" 96%|█████████▌| 9600/10000 [1:25:32<02:32,  2.62it/s]","output_type":"stream"},{"name":"stdout","text":"iter: 009600 - train loss: 1.390 - lr: 4.85e-07 - load time: 0.31 - gpu time: 75.99\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 9800/10000 [1:26:49<01:16,  2.61it/s]","output_type":"stream"},{"name":"stdout","text":"iter: 009800 - train loss: 1.387 - lr: 1.21e-07 - load time: 0.35 - gpu time: 76.21\n","output_type":"stream"},{"name":"stderr","text":"100%|█████████▉| 9999/10000 [1:28:05<00:00,  2.61it/s]","output_type":"stream"},{"name":"stdout","text":"iter: 010000 - train loss: 1.393 - lr: 4.03e-10 - load time: 0.32 - gpu time: 76.27\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10000/10000 [1:30:42<00:00,  1.84it/s]","output_type":"stream"},{"name":"stdout","text":"iter: 010000 - valid loss: 1.589 - acc full seq: 0.0000 - acc per char: 0.0754\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Saved final model to ./weights/vit_transformer.pth\n","output_type":"stream"}],"execution_count":19}]}